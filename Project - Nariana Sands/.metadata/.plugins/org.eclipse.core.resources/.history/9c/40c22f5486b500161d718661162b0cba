import java.util.List;
import java.util.Arrays;
import java.util.Properties;
import java.util.regex.Pattern;

import edu.stanford.nlp.ling.CoreAnnotations.PartOfSpeechAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.SentencesAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TextAnnotation;
import edu.stanford.nlp.ling.CoreAnnotations.TokensAnnotation;
import edu.stanford.nlp.ling.CoreLabel;
import edu.stanford.nlp.pipeline.Annotation;
import edu.stanford.nlp.pipeline.StanfordCoreNLP;
import edu.stanford.nlp.util.CoreMap;

public class POS {
	public static void main(String [] args){
		String featureRequest = args[0].trim(); //Your feature request
		String answer = args[1].trim(); //answer
		String question = args[2]; //question
		int s1, s2, s3; //structured representation of answers
		String s4, s5, s6, s7;
		
		s1 = s1(featureRequest, answer);
		if(s1 == -1){
			System.out.println("Failed to read feature request. Exiting.");
			System.exit(1);
		}
		
		String [] fr /*= featureRequest.split("\\W+")*/; //words in feature request
		String [] a = answer.split("\\W+"); //words in answer
		String [] frPOS; //parts of speech for feature request 

		Properties props = new Properties();
		props.setProperty("ner.useSUTime", "false");
		props.setProperty("annotators", " tokenize, ssplit, pos, lemma, ner, parse, dcoref ");
		StanfordCoreNLP pipeline = new StanfordCoreNLP(props);
		Annotation document = new Annotation(featureRequest);

		pipeline.annotate(document);
		List<CoreMap> sentences = document.get(SentencesAnnotation.class);
		//CoreMap sentence;
		List<CoreLabel> tokens;

		// Stanford Core NLP POS
		//for (CoreMap sentence : sentences) {
		for(int i = 0; i < sentences.size(); i++){
			CoreMap sentence = sentences.get(i);
			tokens = sentence.get(TokensAnnotation.class);
			fr = new String[tokens.size()]; 
			frPOS = new String[tokens.size()]; //feature request parts of speech

			for(int j = 0; j < tokens.size(); j++){
				CoreLabel token = tokens.get(j); //get each token
				fr[j] = token.get(TextAnnotation.class); //get each word
				frPOS[j] = token.get(PartOfSpeechAnnotation.class); //get each POS
				//System.out.print(fr[j] + "/" + frPOS[j] + " ");
			}
			
			s2 = s2(fr, a);
			s3 = s3(a);
			s4 = s4(frPOS,s2);
			s5 = s5(frPOS, s2, s3);
			s6 = s6(frPOS, s2);
			s7 = s7(frPOS, s2, s3);
			System.out.printf("R%d\t%s\t%d\t%d\t%d\t%s\t%s\t%s\t%s\t", i, answer,s1,s2,s3,s4,s5,s6,s7);
			//System.out.println(s1 + ", " + s2 + ", " + s3 + ", \"" + s4 + "\"");
		}
		
		

	}

	public static int s1(String featureRequest, String answer){
		if(Pattern.compile(Pattern.quote(answer), Pattern.CASE_INSENSITIVE).matcher(featureRequest).find())
			return 1; //should be 1 for every sentence
		return -1; //error
	}

	public static int s2(String [] fr, String [] a){
		int s2 = Arrays.asList(fr).indexOf(a[0]);
		if (s2 != -1)
			return s2 +1; //start index at 1
		return s2; //returns -1: error, not found
	}
	
	public static int s3(String [] a){
		return a.length;
	}
	
	public static String s4(String [] frPOS, int s2){
		return frPOS[s2];
	}
	
	public static String s5(String [] frPOS, int s2, int s3){
		return frPOS[s2+s3-1];
	}
	
	public static String s6(String [] frPOS, int s2){
		return frPOS[s2-1];
	}
	
	public static String s7(String [] frPOS, int s2, int s3){
		return frPOS[s2+s3];
	}
}